Experiment 1 setup
network type: Alexnet
learning rate = 0.01

experiment 1:
learning rate 0.001, decay = 0.01
 model = models.alexnet(pretrained=True)
        # Freeze all layers except the last classifier
        for param in model.classifier.parameters():
            param.requires_grad = False
        # Modify the classifier
        model.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(9216, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )

Experimement two parameters
Time 8.30 to 3 - 45mins
network type: Custom network
learning rate = 0.0001
No dropout
input = 150528
hidden = 64
 elif arch == 'custom':
        model = nn.Sequential(
            nn.Linear(150528, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )

 transforms.RandomRotation(30),
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),


Exp 3
learning rate = 0.001
python3 train.py flowers alexnet 256 16
 model.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(9216, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )
Experiment 4
python3 train.py flowers alexnet 256 32
learning rate 0.001
 model = models.alexnet(pretrained=True)
        # Freeze all layers except the last classifier
        for param in model.classifier.parameters():
            param.requires_grad = False
        # Modify the classifier
        model.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(9216, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )
Experiment 5
python3 train.py flowers alexnet 256 64
learning rate 0.001
 model = models.alexnet(pretrained=True)
        # Freeze all layers except the last classifier
        for param in model.classifier.parameters():
            param.requires_grad = False
        # Modify the classifier
        model.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(9216, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )


Next
Experiment 1
   Removed 
        transforms.RandomRotation(30),
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
Next Experiments

Experiment 1:
Did not include: 
a_transforms_train = transforms.Compose([
        # transforms.RandomRotation(30),
        # transforms.RandomResizedCrop(224),
        # transforms.RandomHorizontalFlip(),
python3 train.py flowers alexnet 256 64

Experiment 2:python3 train.py flowers alexnet 128 64

Experiment 3: 
python3 train.py flowers custom 256 64

 model = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(150528, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),
            nn.LogSoftmax(dim=1)
        )
Experiment 4:
python3 train.py flowers vgg16 256 64
  if arch == 'vgg16':
        model = models.vgg16(pretrained=True)
        # Freeze all layers except the last classifier
        for param in model.classifier.parameters():
            param.requires_grad = False
        # Modify the classifier
        model.classifier = nn.Sequential(
            nn.Dropout(0.6),
            nn.Linear(25088, hidden_units),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units // 2, 102),                                                                                      nn.LogSoftmax(dim=1)                                                                                                ) 
